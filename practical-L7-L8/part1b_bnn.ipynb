{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian neural networks with Pyro\n",
    "\n",
    "This notebook uses Pyro to train a Bayesian neural network. This notebook is adapted from: https://github.com/paraschopra/bayesian-neural-network-mnist/blob/master/bnn.ipynb and http://pyro.ai/examples/bayesian_regression.html, written by Ben Moseley.\n",
    "\n",
    "<img src=\"https://pyro.ai/img/pyro_logo.png\" width=\"100\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian neural networks\n",
    "\n",
    "In the previous part, we used Pyro to define probabilistic models in which their random variables represented **physically relevant features** (such as the weight of an object, and the temperature outside) of the world.\n",
    "\n",
    "Another possibility is to define a **black-box** probabilistic model, where its (latent or unobserved) random variables do not have physically salient meanings, but its inputs and outputs nonetheless describe meaningful things about the world.\n",
    "\n",
    "From this viewpoint, we could image a neural network as a probabilistic model, where it's **weights are latent random variables**, and its output is a random variable which depends on the weights and the inputs of the network. More precisely, defining some weights $w$, and input and output variables $x$ and $y$, the neural network is used to model $P(y | w, x)$.\n",
    "\n",
    "A common choice is to assume this distribution is a Normal distribution, such that\n",
    "\n",
    "$\n",
    "P(y | w, x) = \\mathcal{N} (y; \\mu= f(w,x), \\sigma^{2}=1)~,\n",
    "$\n",
    "\n",
    "Where $f(w,x)$ is the output of the neural network. In theory, if we define a **prior** on the neural network weights $w$, we could use Pyro (or any PPL) in exactly the same way as we did in the previous part to infer the posterior distribution of the weights $P(w | \\mathcal{D})$, given some training data (observations) $\\mathcal{D}=\\{(x_1,y_1),...(x_n,y_n)\\}$. A psuedo-function for the Pyro model would be:\n",
    "\n",
    "``` \n",
    "def model(x, y=None):\n",
    "    \"Pyro BNN model\"\n",
    "    \n",
    "    w = pyro.sample('w', some-prior-distribution)\n",
    "    y = pyro.sample('y', dist.Normal(neural-net(w,x), 1), obs=y)\n",
    "    return y\n",
    "```\n",
    "\n",
    "Note, in this approach, $x$ is not considered a random variable (i.e. it is deterministic), but Pyro allows us to pass different values of $x$ to the model by defining it as an input to the stochastic function.\n",
    "\n",
    "In practice, Pyro provides some additional **high-level features** (in particular, `pyro.nn`) that are useful when defining probabilistic models with neural networks which we will use below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The task\n",
    "\n",
    "Our goal is to classify MNIST images. Crucially, instead of just training a normal neural network, we want to design a network which is able to say when it **doesn't know** the right answer (say, for example when we feed the network an image of letter instead of a number). To do this we will train a Bayesian Neural Network (BNN) in Pyro, and output a predictive posterior distribution over the labels. Example images from MNIST are shown below:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"500\" style=\"float: none;\"/>\n",
    "\n",
    "This task consists of multiple stages:\n",
    "\n",
    "- Step 1: load the training/ test data\n",
    "- Step 2: train a standard NN model\n",
    "- Step 3: define a BNN model\n",
    "- Step 4: train the model using SVI\n",
    "- Step 5: validate the results on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: load the training/ test data\n",
    "\n",
    "Below we download the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               ]))\n",
    "\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 1**: plot a selection of the train/ test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: write some code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: train a standard NN model\n",
    "\n",
    "Before we train a BNN, we will train a standard NN classifier to compare our results to. We take the standard approach to classifying MNIST digits using a cross entropy loss, e.g. see here https://github.com/pytorch/examples/blob/master/mnist/main.py : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.log_softmax(x)# output (log) softmax probabilities of each class\n",
    "        return x\n",
    "\n",
    "nn = NN(28*28, 512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 2**: train this neural network over 5 epochs using the loss function and optimizer given below.\n",
    "\n",
    "> **Task 3**: report the average prediction accuracy across the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.NLLLoss()# negative log likelihood loss\n",
    "optimizer = torch.optim.Adam(nn.parameters(), lr=0.01)\n",
    "num_epochs = 5\n",
    "\n",
    "## TODO: write some code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing outside of the training distribution\n",
    "\n",
    "Next, we plot the predictions of the standard NN outside of its training distribution: i.e. on images which we expect the NN should not know the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test letter dataset\n",
    "letters = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\"]\n",
    "x_letters = np.array([np.array(Image.open(\"data/not-mnist/%s.png\"%(l)))\n",
    "                         for l in letters]).astype(np.float32)# loads letters data\n",
    "x_letters /= np.max(x_letters, axis=(1,2), keepdims=True)# normalise\n",
    "x_letters = torch.from_numpy(x_letters)\n",
    "\n",
    "# plot the letters test data\n",
    "plt.figure(figsize=(14,3))\n",
    "for iplot,i in enumerate([0,1,2,3]):\n",
    "    plt.subplot(1,4,iplot+1)\n",
    "    plt.imshow(x_letters[i].numpy())\n",
    "    plt.title(letters[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 4**: plot the predicted softmax probability distribution over each class for each of these test letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TODO: write some code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: define a BNN model\n",
    "\n",
    "It appears the standard network makes *overconfident predictions* on the test data outside of its training distribution.\n",
    "\n",
    "Note, from a probabilistic perspective, the code above defines a neural network which models the labels as a categorical distribution, and the network is trained by maximising the log-likelihood of the training data. i.e., we are making an MLE point estimate of the trainable parameters of the network.\n",
    "\n",
    "This doesn't require specifying a prior over the parameters of the network, nor does it model the full posterior distribution over the parameters (instead it just gives us a point estimate).\n",
    "\n",
    "Instead, we will train a BNN using Pyro to do the same task, this time specifying priors over the parameters, and approximating their full posterior distribution using SVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyroLinear = pyro.nn.PyroModule[torch.nn.Linear]\n",
    "    \n",
    "class BNN(pyro.nn.PyroModule):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = PyroLinear(input_size, hidden_size)\n",
    "        self.fc1.weight = pyro.nn.PyroSample(dist.Normal(0., 1.).expand([hidden_size, input_size]).to_event(2))\n",
    "        self.fc1.bias   = pyro.nn.PyroSample(dist.Normal(0., 1.).expand([hidden_size]).to_event(1))\n",
    "        \n",
    "        self.fc2 = PyroLinear(hidden_size, output_size)\n",
    "        self.fc2.weight = pyro.nn.PyroSample(dist.Normal(0., 1.).expand([output_size, hidden_size]).to_event(2))\n",
    "        self.fc2.bias   = pyro.nn.PyroSample(dist.Normal(0., 1.).expand([output_size]).to_event(1))\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.log_softmax(x)# output (log) softmax probabilities of each class\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Categorical(logits=x), obs=y)\n",
    "            \n",
    "bnn = BNN(28*28, 512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is the Pyro BNN equivalent to the PyTorch NN class defined above.\n",
    "\n",
    "There is a lot going on, and we break this down below:\n",
    "- Instead of defining *stochastic functions*, Pyro allows us to define *stochastic modules* as probabilistic models. We can think of these as an extension of PyTorch's `nn.Module` class, allowing us to use the same idiom but enabling their Bayesian treatment with Pyro.\n",
    "- We can \"Pyro-ize\" an existing PyTorch module by using the `pyro.nn.PyroModule[torch.nn.Linear]` metaclass constructor, or by subclassing `pyro.nn.PyroModule`. All modules in the model need to be \"Pyro-ized\" for this to be a valid Pyro module.\n",
    "\n",
    "We define the network in a very similar way, except for some important changes:\n",
    "- We turn the weights and biases into random variables by using `pyro.nn.PyroSample`: this is the equivalent of `pyro.sample` for creating stochastic `pyro.nn.PyroModule` attributes. Now, every time we call `.forward` above, a new set of weights and biases are sampled from their prior distributions. Note whilst `pyro.sample` requires a name string, Pyro handles the naming of `pyro.nn.PyroSample` attributes automatically.\n",
    "- The `.expand([hidden_size, input_size])` allows us to define multiple univariate distributions which are conditionally independent.\n",
    "- `.to_event(n)` tells Pyro to label the variables as dependent (even though they are in fact independent); `n` is the number of dimensions (http://pyro.ai/examples/tensor_shapes.html#Reshaping-distributions).\n",
    "- The `pyro.plate` context manager allows us to explicitly state that the observations are independent across the batch dimension of the network's inputs, which can speed up the underlying inference.\n",
    "- Note the basic elements of a Pyro model are present: we define some unobserved random variables (the weights and biases of the network), and an observed random variable (the output of the network), using the `obs=` argument.\n",
    "\n",
    "For more on `pyro.nn.PyroModule`, see: http://pyro.ai/examples/tensor_shapes.html and http://pyro.ai/examples/modules.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 5**: Define a SVI guide function to train the BNN. Define separate (independent) univariate normal distributions for each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: the name strings automatically assigned to the BNN's parameters are: `\"fc1.weight\"`, `\"fc1.bias\"`, `\"fc2.weight\"`, and `\"fc2.bias\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(x, y=None):\n",
    "    \n",
    "    ## TODO: write some code here\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoguides\n",
    "\n",
    "Note, Pyro makes defining guides even easier with autoguides: for example, we could have achieved a similar guide using `guide = pyro.infer.autoguide.AutoDiagonalNormal(bnn)` above. See here for more info: http://docs.pyro.ai/en/stable/infer.autoguide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guide = pyro.infer.autoguide.AutoDiagonalNormal(bnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the guide approximation\n",
    "\n",
    "Note, assuming independent distributions for each set of parameters is quite a strong assumption, and could lead to a poor posterior approximation; see here for a discussion: http://pyro.ai/examples/bayesian_regression_ii.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: train the model using SVI\n",
    "\n",
    "Finally, we are ready to train the model. This is very similar to training the models using SVI from the previous part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 6**: train the BNN, using the optimizer and network structure defined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: we can pass training data to SVI using `svi.step(x, y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "svi = SVI(model=bnn, \n",
    "          guide=guide, \n",
    "          optim=Adam({\"lr\": 1e-2}), \n",
    "          loss=Trace_ELBO(num_particles=1))\n",
    "num_epochs = 5\n",
    "\n",
    "## TODO: write some code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 7**: report the average prediction accuracy across the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: we can use `predictive = pyro.infer.Predictive(model=bnn, guide=guide, num_samples=10)` to obtain a function that will sample from the predictive posterior distribution of the BNN, given an input `x` (i.e. it calls .forward using network parameters sampled from the guide). You can take the mode of the predicted class samples to be the network prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: write some code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: validate the results on test data\n",
    "\n",
    "Now we are ready to test the predictions of the BNN on the letters dataset. You should find that the network is much more uncertain about its predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 8**: plot the predictive posterior distribution over each class for each of the test letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: we can again use `predictive` to draw samples from the predictive posterior distribution. You can estimate the distribution by plotting the histogram of counts in each class for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TODO: write some code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible extensions\n",
    "\n",
    "We mentioned above that our SVI implementation makes a strong assumption about the form of the posterior distribution. Here are a few possible extension questions/ tasks:\n",
    "\n",
    "> **(Extension) Question 1**: What other guide functions could you use? How does their computational complexity scale?\n",
    "\n",
    "> **(Extension) Question 2**: How would MCMC methods (e.g. HMC) cope for this problem?\n",
    "\n",
    "> **(Extension) Task 1**: Try experimenting with other guides in: http://docs.pyro.ai/en/stable/infer.autoguide.html  such as `AutoLowRankMultivariateNormal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
