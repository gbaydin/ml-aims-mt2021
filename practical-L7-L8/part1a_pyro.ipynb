{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "planned-grace",
   "metadata": {},
   "source": [
    "# Intro to Pyro\n",
    "\n",
    "This notebook contains a basic introduction to Bayesian modelling and inference in Pyro adapted from: http://pyro.ai/examples/index.html and https://bookdown.org/robertness/causalml/docs/tutorial-on-deep-probabilitic-modeling-with-pyro.html#introduction-to-pyro, written by Ben Moseley.\n",
    "\n",
    "<img src=\"https://pyro.ai/img/pyro_logo.png\" width=\"100\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-playback",
   "metadata": {},
   "source": [
    "# What is a probabilistic programming language?\n",
    "\n",
    "“A probabilistic programming language (PPL) is a programming language designed to describe **probabilistic models** and then perform **inference** in those models. PPLs are closely related to graphical models and Bayesian networks but are more expressive and flexible. Probabilistic programming represents an attempt to”Unify general purpose programming\" with probabilistic modeling.\"\n",
    "\n",
    "-Wikipedia: https://en.wikipedia.org/wiki/Probabilistic_programming\n",
    "\n",
    "A PPL is a domain-specific programming language for that lets you write a data generating process as a program. Similar to causal Bayesian networks, you can write your program in a way that orders the steps of its execution according to cause and effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-virginia",
   "metadata": {},
   "source": [
    "# Pyro\n",
    "\n",
    "Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. Pyro enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling.\n",
    "\n",
    "What this really means: Pyro provides a vast array of modelling and inference abilities, which are super-charged by PyTorch's automatic differentiation and GPU computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-touch",
   "metadata": {},
   "source": [
    "# Bayesian modelling in Pyro\n",
    "\n",
    "In essence, Bayesian inference in PPL (Pyro included) consists of two simple steps:\n",
    "\n",
    "1) Define a probabilistic **model**.\n",
    "\n",
    "2) Carry out **inference** on this model.\n",
    "\n",
    "We will show you how to carry out both of these steps in Pyro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-vulnerability",
   "metadata": {},
   "source": [
    "# 1) Defining a model (aka, stochastic function)\n",
    "\n",
    "In Pyro, a model is defined using a **\"stochastic function\"**.\n",
    "\n",
    "A stochastic function is an arbitrary Python callable (function or method) that combines two ingredients:\n",
    "\n",
    "- Deterministic Python code\n",
    "- Primitive stochastic functions that call a random number generator.\n",
    "\n",
    "## Primitive stochastic functions\n",
    "\n",
    "These are basic elements which allow us to introduce stochasticity into our model, and essentially allow us to define different **distributions** in our model. The code below defines a normal distribution and samples from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 0.   # mean zero\n",
    "scale = 1. # unit variance\n",
    "normal = torch.distributions.Normal(loc, scale) # create a normal distribution object\n",
    "x = normal.rsample() # draw a sample from N(0,1)\n",
    "print(\"sample: \", x, type(x))# returns a pytorch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-hundred",
   "metadata": {},
   "source": [
    "Pyro provides a thin wrapper around `torch.distributions`, to simplify the process of defining and sampling from distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "\n",
    "x = pyro.sample(\"my_sample\", pyro.distributions.Normal(loc, scale))\n",
    "print(x, type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-pricing",
   "metadata": {},
   "source": [
    "Just like a direct call to `torch.distributions.Normal().rsample()`, `pyro.sample` returns a sample from the unit normal distribution. The crucial difference is that this sample is **named**. Pyro’s backend uses these names to uniquely identify sample statements and change their behavior at runtime depending on how the enclosing stochastic function is being used. This is how Pyro can implement the various manipulations that underlie inference algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-america",
   "metadata": {},
   "source": [
    "## A simple model\n",
    "\n",
    "Now we are ready to write our first probabilistic model! All probabilistic programs are built up by composing primitive stochastic functions and deterministic code. Let's write a simple `weather` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather():\n",
    "    \"Our Pyro model of the weather\"\n",
    "    \n",
    "    # define cloudy as a random variable with a Bernoulli prior distribution\n",
    "    cloudy = pyro.sample('cloudy', dist.Bernoulli(0.3))\n",
    "    \n",
    "    # define temperature as a random variable, through its conditional distribution with cloudy\n",
    "    weather_type = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    loc = {'cloudy': 55.0, 'sunny': 75.0}[weather_type]\n",
    "    scale = {'cloudy': 10.0, 'sunny': 15.0}[weather_type]\n",
    "    temperature = pyro.sample('temperature', dist.Normal(loc, scale))\n",
    "    \n",
    "    return weather_type, temperature\n",
    "\n",
    "for _ in range(3):\n",
    "    weather_type, temperature = weather()\n",
    "    print(\"The weather is: %s and the temperature is: %.2f\"%(weather_type, temperature.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-coach",
   "metadata": {},
   "source": [
    "We note a few important things about our model here:\n",
    "- We defined two random variables in our model: the first is `cloudy` defined through its prior distribution (i.e. $P(cloudy)$), and the second is `temperature`, which depends on `cloudy` and is defined through its conditional distribution with `cloudy` (i.e. $P(temperature | cloudy)$). (Note the similarity to causal Bayesian networks!).\n",
    "- According to our model, the weather is cloudy 30% of the time, and sunny the rest. On sunny days, the average temperature is higher than on cloudy days.\n",
    "- We are able to add complex **conditional logic** to the model: the mean and standard deviation of `temperature` depends discontinuously on the value of `cloudy`.\n",
    "- The model is evaluated **dynamically**: samples (PyTorch tensors) from the model are drawn during the execution of the function (i.e. the model is not statically defined before it is sampled), just like in normal PyTorch code.\n",
    "- Because we use `pyro.sample` and `pyro.distributions`, `weather` defines a probabilistic model that we can carry out inference over (see later).\n",
    "\n",
    "We can build complex models by modularizing and reusing such functions, using them as programmers use functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ice_cream_sales():\n",
    "    \"Another Pyro model\"\n",
    "    \n",
    "    # define ice_cream as a random variable, through its conditional distribution with cloudy and temperature\n",
    "    weather_type, temperature = weather()\n",
    "    expected_sales = 200. if weather_type == 'sunny' and temperature > 80.0 else 50.\n",
    "    ice_cream = pyro.sample('ice_cream', pyro.distributions.Normal(expected_sales, 10.0))# number of ice cream sales\n",
    "    return weather_type, ice_cream\n",
    "\n",
    "print(ice_cream_sales())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-divide",
   "metadata": {},
   "source": [
    "This kind of modularity, familiar to any programmer, is obviously very powerful. But is it powerful enough to encompass all the different kinds of models we’d like to express?\n",
    "\n",
    "It turns out that because Pyro is embedded in Python, stochastic functions can contain arbitrarily complex deterministic Python and randomness can freely affect control flow. For example, we can even construct recursive functions that terminate their recursion nondeterministically, provided we take care to pass `pyro.sample` unique sample names whenever it’s called. Check out: http://pyro.ai/examples/intro_part_i.html for more examples of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-authentication",
   "metadata": {},
   "source": [
    "> **Task 1**: draw 1,000 samples of `ice_cream` from the `ice_cream_sales` model, and plot separate histograms of the expected number of ice cream sales for sunny and cloudy days respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## TODO: write some code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-convergence",
   "metadata": {},
   "source": [
    "# 2) Inference\n",
    "\n",
    "As we discussed earlier, the reason we use PPLs is because they can easily go backwards and reason about cause given the observed effect, i.e. they can carry out **inference**. There are myriad of inference algorithms available in Pyro.\n",
    "\n",
    "## An even simpler model\n",
    "\n",
    "To demonstrate inference in Pyro, we'll use an even simpler model. Consider trying to estimate the weight, $w$, of some object (In probabilistic language, the weight here is a latent, or **unobserved random variable** we would like to infer).\n",
    "\n",
    "Consider also that we also have an unreliable measuring scale, which measures the weight of the object as $m$.\n",
    "\n",
    "Then, let's assume that the probability of a measurement given the weight of the object is given by:\n",
    "\n",
    "$\n",
    "P(m | w) = \\mathcal{N}(m; \\mu=w, \\sigma^{2}=0.75)\n",
    "$\n",
    "\n",
    "Also consider the situation where we know the prior distribution of the object's weight (for example, we weighed it in our hand before placing it on the scale):\n",
    "\n",
    "$\n",
    "P(w) = \\mathcal{N}(w; \\mu=10, \\sigma^{2}=1)\n",
    "$\n",
    "\n",
    "In Pyro, we can write this model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale():\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(10, 1.0))\n",
    "    measurement = pyro.sample(\"measurement\", dist.Normal(weight, 0.75))\n",
    "    return measurement\n",
    "print(scale())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-consensus",
   "metadata": {},
   "source": [
    "Now, given the value of the measurement $m$ (i.e. an **observation**), we want to infer the **posterior distribution** of the weight of the object, i.e.\n",
    "\n",
    "$P(w | m)$\n",
    "\n",
    "Which, by Bayes rule, is given by\n",
    "\n",
    "$P(w | m) = {P(m | w) P(w) \\over P(m)}\n",
    "$\n",
    "\n",
    "## Adding observed data (aka, conditioning) in Pyro\n",
    "\n",
    "To obtain the posterior distribution in Pyro, we first need a way to feed in **observations** (i.e. the scale measurement value) to our model.\n",
    "\n",
    "In Pyro this is known as \"conditioning\", and is done using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned_scale = pyro.condition(scale, data={\"measurement\": torch.tensor(14.)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-education",
   "metadata": {},
   "source": [
    "`pyro.condition` is a higher-order function that takes a model and a dictionary of observations and returns a new model that has the same input and output signatures but always uses the given values at observed `sample` statements.\n",
    "\n",
    "Another, alternative way which is sometimes easier is to pass observations directly to individual `pyro.sample` statements, using the optional `obs` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_obs():  # equivalent to conditioned_scale above\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(10, 1.0))\n",
    "    measurement = pyro.sample(\"measurement\", dist.Normal(weight, 0.75), obs=14.)\n",
    "    return measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-turning",
   "metadata": {},
   "source": [
    "## Inference using MCMC\n",
    "\n",
    "Now, all we need to do is use an inference algorithm to estimate the posterior distribution of the unobserved random variables in our model (in this case, the weight of the object), given the observed data.\n",
    "\n",
    "There are many different ways to do this in Pyro (see here: http://docs.pyro.ai/en/stable/inference.html ); one is to use Hamiltonian Monte Carlo sampling to draw samples from the posterior: (http://docs.pyro.ai/en/stable/mcmc.html )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.mcmc import MCMC\n",
    "from pyro.infer.mcmc.nuts import HMC\n",
    "\n",
    "hmc_kernel = HMC(conditioned_scale, step_size=0.9, num_steps=4)# defines a HMC kernel\n",
    "sampler = MCMC(hmc_kernel, # uses MCMC with HMC kernel\n",
    "               num_samples=1000, # generates 1000 samples\n",
    "               warmup_steps=50)# burn-in of 50 steps\n",
    "sampler.run()\n",
    "sampler.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-phone",
   "metadata": {},
   "source": [
    "We can use `sampler.get_samples()` to obtain the resulting posterior samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-applicant",
   "metadata": {},
   "source": [
    "> **Task 2**: plot the samples from the posterior distribution as a histogram below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = sampler.get_samples()\n",
    "print(type(posterior))# note, this is just a python dictionary!\n",
    "\n",
    "## TODO: write some code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-tobago",
   "metadata": {},
   "source": [
    "## Inference using variational inference\n",
    "\n",
    "Instead of using MCMC to draw samples from the posterior, another option is to use **variational inference** to fit an (approximate) distribution $Q$ to the true posterior distribution of the unobserved variables, given the observed variable values (http://docs.pyro.ai/en/stable/inference_algos.html ).\n",
    "\n",
    "First, we need to define a flexible approximate distribution $Q$, or **guide function** as it is called in Pyro, to fit to the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide():\n",
    "    \"Defines an approximate distribution Q we will fit to the posterior\"\n",
    "    \n",
    "    # here, we define Q as a normal distribution, with flexible (trainable) parameters a and b\n",
    "    a = pyro.param(\"a\", torch.tensor(1.))\n",
    "    b = pyro.param(\"b\", torch.tensor(1.))\n",
    "    return pyro.sample(\"weight\", dist.Normal(a, torch.abs(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-control",
   "metadata": {},
   "source": [
    "Importantly, guide functions must satisfy these two criteria to be valid approximations for a particular model:\n",
    "\n",
    "1. all unobserved (i.e., not conditioned) sample statements that appear in the model appear in the guide. Concretely, if the model contains `pyro.sample(\"z_1\", ...)`, where `\"z_1\"` is unobserved, then the guide must also contain `pyro.sample(\"z_1\", ...)`.\n",
    "2. the guide has the same input signature as the model (i.e., takes the same arguments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-listing",
   "metadata": {},
   "source": [
    "There are a few things to note here:\n",
    "- Here, we define Q to be a normal distribution, with its mean and standard deviation as trainable parameters.\n",
    "- We use `pyro.param` to define the trainable parameters in Q. Pyro `Parameters` are a thin wrapper over `torch.Tensor`, and, (similar to `pyro.sample`), `pyro.param` crucially assigns a name to the parameter, to allow the underlying inference algorithm to keep track of it.\n",
    "- Also note we had to apply `torch.abs` to parameter `b` because the standard deviation of a normal distribution has to be positive; similar restrictions also apply to parameters of many other distributions.\n",
    "- Pyro `Parameters` automatically start tracking the gradient of their underlying Torch tensor, which will allow Pyro to use automatic differentiation and gradient descent on loss functions used for inference to update them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pyro.param(\"new_param\", torch.tensor(1.))\n",
    "print(a, a.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-preparation",
   "metadata": {},
   "source": [
    "Although the precise meaning of the guide is different across different inference algorithms, the guide function should generally be chosen so that, in principle, it is *flexible enough to closely approximate the posterior distribution over all unobserved variables in the model*. It turns out, for the `scale` model above, the posterior can be analytically derived as a Normal distribution, so we are good in this case (in general it is usually intractable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-archive",
   "metadata": {},
   "source": [
    "> (Optional) **Extension Task 1**: show analytically that for the `scale` model, the posterior distribution is a Normal distribution and derive its mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"True analytical posterior parameters:\")\n",
    "print(\"loc = \", (0.75**2 * 10 + 14.) / (1 + 0.75**2))\n",
    "print(\"scale = \", np.sqrt(0.75**2/(1 + 0.75**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-spotlight",
   "metadata": {},
   "source": [
    "### More on pyro.param\n",
    "\n",
    "`pyro.param` is actually a frontend for Pyro’s key-value parameter store (see the documentation for more detail). Importantly, the first time `pyro.param` is called with a particular name, it stores its argument in the parameter store and then returns that value. After that, when it is called with that name, it returns the value from the parameter store regardless of any other arguments.\n",
    "\n",
    "You can call `pyro.clear_param_store()` to clear the parameter store before initialising a parameter again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pyro.param(\"new_param\", torch.tensor(10.))# value of 10. is ignored (!)\n",
    "print(a, a.requires_grad)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "a = pyro.param(\"new_param\", torch.tensor(10.))\n",
    "print(a, a.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-pension",
   "metadata": {},
   "source": [
    "## Training Q\n",
    "\n",
    "Now, we are ready to fit the approximate posterior distribution to the true posterior distribution. To do this, we will maximise the Evidence Lower Bound, or ELBO, using stochastic gradient descent. This essentially minimises the Kullback–Leibler divergence between the true posterior and Q.\n",
    "\n",
    "In the code below, Pyro computes Monte Carlo estimates of the ELBO, and the guide parameters are updated using SGD. For more details, see: http://pyro.ai/examples/svi_part_i.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import SGD\n",
    "\n",
    "pyro.clear_param_store()\n",
    "svi = SVI(model=conditioned_scale,\n",
    "          guide=guide,\n",
    "          optim=SGD({\"lr\": 0.001, \"momentum\":0.1}),\n",
    "          loss=Trace_ELBO(num_particles=1))# num_particles is the number of MC samples used to estimate ELBO\n",
    "\n",
    "losses, a,b  = [], [], []\n",
    "num_steps = 2500\n",
    "for i in range(num_steps):\n",
    "    losses.append(svi.step())\n",
    "    a.append(pyro.param(\"a\").item())\n",
    "    b.append(pyro.param(\"b\").item())\n",
    "    \n",
    "print('a = ', a[-1])\n",
    "print('b = ', b[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-district",
   "metadata": {},
   "source": [
    "> **Task 3**: plot the values of the ELBO, `a` and `b` as a function of training step.\n",
    "\n",
    "> **Task 4**: plot the posterior distribution by drawing samples from Q. Compare this to the posterior samples estimated from HMC above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-hydrogen",
   "metadata": {},
   "source": [
    "Note that optimization will update the values of the guide parameters in the parameter store, so that once we find good parameter values, we can use samples from the guide as posterior samples for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: write some code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-continent",
   "metadata": {},
   "source": [
    "Note that both SVI and HMC estimate posterior distributions which closely match the analytical solution. This is expected for SVI as Q is drawn from the same function family as the analytical posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-expression",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "There is so much more that is possible with Pyro; if you truly want to explore the power of Pyro, we highly recommend browsing through the official Pyro tutorials here: https://pyro.ai/examples/index.html#\n",
    "\n",
    "There are examples of using Pyro for deep generative models, MLE and MAP estimation, time series modelling, Gaussian processes, ...\n",
    "\n",
    "We also recommend reading `minipyro`, a stripped down version of Pyro which will help you understand its internals: https://pyro.ai/examples/minipyro.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-cliff",
   "metadata": {},
   "source": [
    "## A note on using GPUs\n",
    "Combining stochastic gradient descent with PyTorch’s GPU-accelerated tensor math and automatic differentiation allows Pyro to scale variational inference to very high-dimensional parameter spaces and massive datasets.\n",
    "\n",
    "Although not covered here, using CUDA in Pyro is very similar to using CUDA in standard PyTorch code: you just need to make sure all tensors (and any neural network models) are converted to CUDA types, see here for an example: https://github.com/pyro-ppl/pyro/blob/dev/examples/vae/vae.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f110ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
